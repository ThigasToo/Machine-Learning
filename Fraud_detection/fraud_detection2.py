# -*- coding: utf-8 -*-
"""Fraud_detection2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ytkwNfC0neavHnmPVrrSLE5DmbjXuRa3
"""

!pip install imbalanced-learn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
sns.set(style='whitegrid')

df = pd.read_csv("AIML Dataset.csv")

df.head()

df.info()

df.columns

df['isFraud'].value_counts()

df['isFlaggedFraud'].value_counts()

df.isnull().sum().sum()

df.shape

#porcentage of fraud in our dataset
round((df['isFraud'].value_counts()[1] / df.shape[0]) *100, 2)

df["type"].value_counts().plot(kind="bar", title="Transaction Types", color="green")
plt.xlabel("Transaction Type")
plt.ylabel("Count")
plt.show()

fraud_by_type

fraud_by_type = df.groupby(["type"])["isFraud"].mean().sort_values(ascending=False)
fraud_by_type.plot(kind="bar", title="Fraud Rate by Type", color="red")
plt.xlabel("Fraud Rate")
plt.show()

df['amount'].describe().astype(int)

sns.histplot(np.log1p(df['amount']), bins=100, kde=True, color='orange')
plt.title("Transaction Amount Distribution (long scale)")
plt.xlabel("Amount + 1")
plt.show()

sns.boxplot(data = df[df['amount'] < 50000], x = "isFraud", y="amount")
plt.title("Amount vs isFraud (Filtered under 50k)")
plt.show()

df["balanceDiffOrig"] = df["oldbalanceOrg"] - df["newbalanceOrig"]
df["balanceDiffDest"] = df["newbalanceDest"] - df["oldbalanceDest"]

(df['balanceDiffOrig'] < 0).sum()

(df['balanceDiffDest'] < 0).sum()

df.head(2)

# to see if its time dependent
frauds_per_step = df[df["isFraud"] == 1] ["step"].value_counts().sort_index()
plt.plot(frauds_per_step.index, frauds_per_step.values, label= "Frauds per step")
plt.xlabel("Step (time)")
plt.ylabel("Number of frauds")
plt.title("Frauds per step")
plt.show()

#it is non time dependent

df.drop(columns= "step", inplace=True)

df.head(2)

top_senders = df["nameOrig"].value_counts().nlargest(10)
top_senders2 = df["nameOrig"].value_counts().head(10)
top_receivers = df["nameDest"].value_counts().nlargest(10)

top_senders, top_senders2

top_receivers

fraud_users = df[df["isFraud"] == 1]["nameOrig"].value_counts().nlargest(10)
fraud_users

fraud_types = df[df["type"].isin(["CASH_OUT", "TRANSFER"])]
fraud_types

fraud_types["type"].value_counts()

sns.countplot(data=fraud_types, x="type", hue="isFraud")
plt.title("Fraud Distribution in Transfer & Cashout")
plt.show()

corr = df[["amount", "oldbalanceOrg", "newbalanceOrig", "oldbalanceDest", "newbalanceDest", "isFraud"]].corr()

corr

sns.heatmap(corr, annot=True, cmap = "coolwarm", fmt = ".2f")
plt.title("Correlation Heatmap")
plt.show()

zero_after_transfer = df[
    (df["oldbalanceOrg"] > 0) &
    (df["newbalanceOrig"] == 0) &
    (df["type"].isin(["TRANSFER", "CASH_OUT"]))
]

len(zero_after_transfer)

zero_after_transfer.head()

df["isFraud"].value_counts()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
#from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline # Importante: usar o Pipeline do imblearn!
from sklearn.ensemble import RandomForestClassifier

df.head()

df_model = df.drop(["nameOrig", "nameDest", "isFlaggedFraud"], axis = 1)

df_model.head()

categorical = ["type"]
numeric = ["amount", "oldbalanceOrg", "newbalanceOrig", "oldbalanceDest", "newbalanceDest"]

y = df_model["isFraud"]
X = df_model.drop("isFraud", axis = 1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)

preprocessor = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(), numeric),
        ("cat", OneHotEncoder(drop="first"), categorical)
    ], # Added a comma here
    remainder = "drop"
)

pipeline = ImbPipeline([
    ("preprocessor", preprocessor), # applying the preprocessor before the SMOTE technique
    ("smote", SMOTE(random_state=42)),
    ("classifier", RandomForestClassifier(random_state=42, n_jobs=-1))
])

            #Pipeline([
   # ("preprocessor", preprocessor),
   # ("classifier", LogisticRegression(class_weight="balanced"), max_iter=1000)])

#With smote, what to expect: When running the classification_report again, you should notice a significant improvement in the metrics for class 1 (fraud), especially in recall and precision, and consequently in the f1-score.The overall accuracy may even decrease slightly, but don’t worry! This is normal. The previous accuracy was artificially high because the model ignored the minority class. The new f1-score for the fraud class will be a much more reliable indicator of your model's success.

pipeline.fit(X_train, y_train)

y_pred = pipeline.predict(X_test)

print(classification_report(y_test, y_pred))
#let´s see if the precision is better

confusion_matrix(y_test, y_pred)

pipeline.score(X_test, y_test) * 100

import joblib

joblib.dump(pipeline, "fraud_detection_pipeline.pkl")